{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://68.media.tumblr.com/79dbb18434f983637e706857067b2fab/tumblr_nzeqww3Lep1tcvh9jo1_400.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "from preprocessing import *\n",
    "from math_utils import *\n",
    "from plotting import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 6\n",
    "\n",
    "IMAGES_PATH = 'train-images-idx3-ubyte'\n",
    "LABELS_PATH = 'train-labels-idx1-ubyte'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "N_FEATURES = 28 * 28\n",
    "N_CLASSES = 10\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "## The sigmoid function (and it's derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10., 10., num=100)\n",
    "sig = sigmoid(x)\n",
    "sig_prime = sigmoid_prime(x)\n",
    "\n",
    "plt.plot(x, sig, label=\"sigmoid\")\n",
    "plt.plot(x, sig_prime, label=\"sigmoid prime\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(prop={'size' : 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "The softmax function can be easily differentiated, it is pure (output depends only on input) and the elements of the resulting vector sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(np.array([[2, 4, 6, 8]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n",
    "Let's define 2 matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "m2 = np.array([[3, 2, 1], [4, 3, 2]])\n",
    "\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.dot(m2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "The algorithm consists of 3 subtasks:\n",
    "\n",
    "* Make a forward pass\n",
    "* Calculate the error\n",
    "* Make backward pass (backpropagation)\n",
    "\n",
    "In the first step, backprop uses the data and the weights of the network to compute a prediction. Next, the error is computed based on the prediction and the provided labels. The final step propagates the error through the network, starting from the final layer. Thus, the weights get updated based on the error, little by little.\n",
    "\n",
    "We will try to create a Neural Network (NN) that can properly predict values from the *XOR* function:\n",
    "\n",
    "| Input 1 \t| Input 2 \t| Output \t|\n",
    "|---------\t|---------\t|--------\t|\n",
    "| 0       \t| 0       \t| 0      \t|\n",
    "| 0       \t| 1       \t| 1      \t|\n",
    "| 1       \t| 0       \t| 1      \t|\n",
    "| 1       \t| 1       \t| 0      \t|\n",
    "\n",
    "*XOR* can be plotted as:\n",
    "\n",
    "<img src=\"https://wantee.github.io/assets/images/posts/xor.png\" width=\"60%\">\n",
    "\n",
    "Let start by defining some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50000\n",
    "input_size, hidden_size, output_size = 2, 3, 1\n",
    "LR = .1 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([ [0],   [1],   [1],   [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights of our NN to random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_hidden = np.random.uniform(size=(input_size, hidden_size))\n",
    "w_output = np.random.uniform(size=(hidden_size, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implementation of the Backprop algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    " \n",
    "    # Forward\n",
    "    act_hidden = sigmoid(np.dot(X, w_hidden))\n",
    "    output = np.dot(act_hidden, w_output)\n",
    "    \n",
    "    # Calculate error\n",
    "    error = y - output\n",
    "    \n",
    "    if epoch % 5000 == 0:\n",
    "        print(f'error sum {sum(error)}')\n",
    "\n",
    "    # Backward\n",
    "    dZ = error * LR\n",
    "    w_output += act_hidden.T.dot(dZ)\n",
    "    dH = dZ.dot(w_output.T) * sigmoid_prime(act_hidden)\n",
    "    w_hidden += X.T.dot(dH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict using our trained model (doing just the forward step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X[1] # [0, 1]\n",
    "\n",
    "act_hidden = sigmoid(np.dot(X_test, w_hidden))\n",
    "np.dot(act_hidden, w_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why NNs use biases?](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = read_mnist(IMAGES_PATH, LABELS_PATH)\n",
    "X, y = shuffle_data(X, y, random_seed=RANDOM_SEED)\n",
    "X_train, y_train = X[:500], y[:500]\n",
    "X_test, y_test = X[500:], y[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit(X, y, idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNClassifier:\n",
    "\n",
    "    def __init__(self, n_classes, n_features, n_hidden_units=30,\n",
    "                 l1=0.0, l2=0.0, epochs=500, learning_rate=0.01,\n",
    "                 n_batches=1, random_seed=None):\n",
    "\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.w1, self.w2 = self._init_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_batches = n_batches\n",
    "\n",
    "    def _init_weights(self):\n",
    "        w1 = np.random.uniform(-1.0, 1.0, \n",
    "                               size=self.n_hidden_units * (self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden_units, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0, \n",
    "                               size=self.n_classes * (self.n_hidden_units + 1))\n",
    "        w2 = w2.reshape(self.n_classes, self.n_hidden_units + 1)\n",
    "        return w1, w2\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0]+1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('how must be columns or row')\n",
    "        return X_new\n",
    "\n",
    "    def _forward(self, X):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _backward(self, net_input, net_hidden, act_hidden, act_out, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _error(self, y, output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _backprop_step(self, X, y):\n",
    "        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(X)\n",
    "        y = y.T\n",
    "\n",
    "        grad1, grad2 = self._backward(net_input, net_hidden, act_hidden, act_out, y)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += (self.w1[:, 1:] * (self.l1 + self.l2))\n",
    "        grad2[:, 1:] += (self.w2[:, 1:] * (self.l1 + self.l2))\n",
    "\n",
    "        error = self._error(y, act_out)\n",
    "        \n",
    "        return error, grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.error_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_data_enc = one_hot(y_data, self.n_classes)\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            X_mb = np.array_split(X_data, self.n_batches)\n",
    "            y_mb = np.array_split(y_data_enc, self.n_batches)\n",
    "\n",
    "            for Xi, yi in zip(X_mb, y_mb):\n",
    "                \n",
    "                # update weights\n",
    "                error, grad1, grad2 = self._backprop_step(Xi, yi)\n",
    "                self.error_.append(error)\n",
    "                self.w1 -= (self.learning_rate * grad1)\n",
    "                self.w2 -= (self.learning_rate * grad2)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return np.sum(y == y_hat, axis=0) / float(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NNClassifier(n_classes=N_CLASSES, \n",
    "                  n_features=N_FEATURES,\n",
    "                  n_hidden_units=50,\n",
    "                  l2=0.5,\n",
    "                  l1=0.0,\n",
    "                  epochs=300,\n",
    "                  learning_rate=0.001,\n",
    "                  n_batches=25,\n",
    "                  random_seed=RANDOM_SEED)\n",
    "\n",
    "nn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Accuracy: %.2f%%' % (nn.score(X_train, y_train) * 100))\n",
    "print('Test Accuracy: %.2f%%' % (nn.score(X_test, y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict_proba(X_test[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit(X_test, y_test, idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit_dist(X_test, y_test, idx=1, model=nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrong prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit(X_test, y_test, idx=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit_dist(X_test, y_test, idx=25, model=nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE = picking the most probable digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict_proba(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle(nn.predict_proba(X_test[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict(X_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
