{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras import backend as K\n",
    "import random\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(fig_id + \".png\", format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = env.render(mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "save_fig(\"Breakout\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "n_max_steps = 1000\n",
    "n_change_steps = 10\n",
    "\n",
    "obs = env.reset()\n",
    "for step in range(n_max_steps):\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    if step % n_change_steps == 0:\n",
    "        action = env.action_space.sample() # play randomly\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, \n",
    "                                   fargs=(frames, patch), frames=len(frames), \n",
    "                                   repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(pre_processing(obs), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original observation (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed observation (84×84 greyscale)\")\n",
    "plt.imshow(pre_processing(obs), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "save_fig(\"preprocessing_plot\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EPISODES = 50000\n",
    "EPISODES = 1\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) / 1000000.\n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 10000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.optimizer = self.optimizer()\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "\n",
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None, ), dtype='int32')\n",
    "        y = K.placeholder(shape=(None, ), dtype='float32')\n",
    "\n",
    "        py_x = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(py_x * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # approximate Q function using Convolution Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=self.state_size))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def replay_memory(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append((history, action, reward, next_history, dead))\n",
    "\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.zeros((self.batch_size, self.state_size[0], self.state_size[1], self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size, self.state_size[0], self.state_size[1], self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size, ))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "        \n",
    "        # like Q Learning, get maximum Q value at s'\n",
    "        # But from target model\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * np.amax(target_value[i])\n",
    "\n",
    "        loss = self.optimizer([history, action, target])\n",
    "        self.avg_loss += loss[0]\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(action_size)\n",
    "\n",
    "scores, episodes, global_step = [], [], 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    dead = False\n",
    "    # 1 episode = 5 lives\n",
    "    step, score, start_life = 0, 0, 5\n",
    "    observe = env.reset()\n",
    "\n",
    "    # this is one of DeepMind's idea.\n",
    "    # just do nothing at the start of episode to avoid sub-optimal\n",
    "    for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "        observe, _, _, _ = env.step(1)\n",
    "\n",
    "    # At start of episode, there is no preceding frame. So just copy initial states to make history\n",
    "    state = pre_processing(observe)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "    while not done:\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "\n",
    "        # get action for the current history and go one step in environment\n",
    "        action = agent.get_action(history)\n",
    "        observe, reward, done, info = env.step(action)\n",
    "        # pre-process the observation --> history\n",
    "        next_state = pre_processing(observe)\n",
    "        next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "        next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "        agent.avg_q_max += np.amax(agent.model.predict(np.float32(history / 255.))[0])\n",
    "\n",
    "        # if the ball is fall, then the agent is dead --> episode is not over\n",
    "        if start_life > info['ale.lives']:\n",
    "            dead = True\n",
    "            start_life = info['ale.lives']\n",
    "\n",
    "        reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.replay_memory(history, action, reward, next_history, dead)\n",
    "        # every some time interval, train model\n",
    "        agent.train_replay()\n",
    "        # update the target model with model\n",
    "        if global_step % agent.update_target_rate == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # if agent is dead, then reset the history\n",
    "        if dead:\n",
    "            dead = False\n",
    "        else:\n",
    "            history = next_history\n",
    "\n",
    "        # if done, plot the score over episodes\n",
    "        if done:\n",
    "            print(\"\\repisode:\", e, \"  score:\", score, \"  memory length:\", len(agent.memory),\n",
    "                  \"  epsilon:\", agent.epsilon, \"  global_step:\", global_step, \"  average_q:\", agent.avg_q_max/float(step),\n",
    "                  \"  average loss:\", agent.avg_loss/float(step), end=\"\")\n",
    "\n",
    "            agent.avg_q_max, agent.avg_loss = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.load_model('./Breakout_DQN.h5')\n",
    "# agent.epsilon = 0.0\n",
    "frames = []\n",
    "for _ in range(5):\n",
    "    done = False\n",
    "    dead = False\n",
    "\n",
    "    step, score, start_life = 0, 0, 5\n",
    "    observe = env.reset()\n",
    "\n",
    "    # this is one of DeepMind's idea.\n",
    "    # just do nothing at the start of episode to avoid sub-optimal\n",
    "    for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "        observe, _, _, _ = env.step(1)\n",
    "\n",
    "    # At start of episode, there is no preceding frame. So just copy initial states to make history\n",
    "    state = pre_processing(observe)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "\n",
    "        action = agent.get_action(history)\n",
    "        observe, reward, done, info = env.step(action)\n",
    "        # pre-process the observation --> history\n",
    "        next_state = pre_processing(observe)\n",
    "        next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "        next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "        if start_life > info['ale.lives']:\n",
    "            dead = True\n",
    "            start_life = info['ale.lives']\n",
    "\n",
    "        if dead:\n",
    "            dead = False\n",
    "        else:\n",
    "            history = next_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = plot_animation(frames);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
