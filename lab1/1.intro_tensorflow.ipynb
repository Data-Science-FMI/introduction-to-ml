{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://curiousily.com/assets/11.tensorflow_for_hackers_part_1_files/tensors_flowing.gif\" width=\"50%\" height=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs\n",
    "\n",
    "Everything you do in TensorFlow is based on computational graphs. Each node of the graph defines an operation. The operation can be any function - addition, multiplication or some funky custom function you wrote.\n",
    "\n",
    "## Tensors\n",
    "\n",
    "You can give a scalar value, vector, matrix or a Tensor to the operations in the graph. Tensors are just multi-dimensional matrices.\n",
    "\n",
    "Let's have a look at an example computational graph:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*mvhm5_r6LY-eHsin21RJTg.png\" height=\"80%\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this look like in TensorFlow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t1 = tf.Variable(2)\n",
    "t2 = tf.Variable(3)\n",
    "\n",
    "res = t1 * t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy, right? No graphs, nothing seriously strange. And now for the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That must be how new math is developed. $2 \\times 3 = \\text{tf.Tensor}$?\n",
    "\n",
    "First, let's see the hidden `default` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All your Tensors are attached to the mystical `default` graph. What about the result?\n",
    "\n",
    "# Sessions\n",
    "\n",
    "See, the code above didn't perform any computation. You just created a computational graph. If you want to do something meaningful you need a `session`. Sessions are used for graph evaluation, think variable initialization and actually computing `res`.\n",
    "\n",
    "A TensorFlow session takes care of running operations (on GPUs and CPUs) and holding variable values. Let's see how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    t1.initializer.run()\n",
    "    t2.initializer.run()\n",
    "    print(f\"Multiplication result: {res.eval()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialized all our tensors and evaluated the result. Here is a somewhat shorter version that initializes all global variables automagically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # or sess.run(init)\n",
    "    print(f\"Multiplication result: {res.eval()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n",
    "\n",
    "Variables are just... eh, variables? By definition, you can change their values. That is quite useful, since tensors and operators are immutable. Let's create a normally distributed $3\\times4$ matrix with $mean=0$ and $std=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal = tf.Variable(tf.random_normal([3, 4], mean=0.0, stddev=100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(normal.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal = normal.assign(tf.Variable(tf.random_normal([3, 4], mean=0.0, stddev=100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(normal.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard, right?\n",
    "\n",
    "# Simple Linear Regression\n",
    "\n",
    "$$Y = aX + b$$\n",
    "\n",
    "Where Y is the dependent and X is the independent variable. Our task is to adjust the parameters a - “slope” and b - “intercept” so that we best describe the data using a line.\n",
    "\n",
    "For our example, let’s find out how eating burgers affect your resting heart rate. The data will be simulated, so no conclusions, please! Our data represents the average number of burgers eaten per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = np.random.rand(100).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope and intercept we are looking for are respectively $a=50$ and $b=40$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 50\n",
    "b = 40\n",
    "Y = a * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at what our model should figure out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make things a tiny bit more interesting by adding a bit of noise to our dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.vectorize(lambda y: y + np.random.normal(loc=0.0, scale=0.05))(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow for Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a_var = tf.Variable(1.0)\n",
    "b_var = tf.Variable(1.0)\n",
    "y_var = a_var * X + b_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task will be to minimize the mean squared error or in TensorFlow parlance - reduce the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y_var - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let’s try to minimize it using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use our optimizer for 300 steps of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 300\n",
    "results = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(TRAINING_STEPS):\n",
    "        results.append(sess.run([train, a_var, b_var])[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get the final and best predictions for $a$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = results[-1]\n",
    "a_hat = final_pred[0]\n",
    "b_hat = final_pred[1]\n",
    "y_hat = a_hat * X + b_hat\n",
    "\n",
    "print(f\"a_hat: {a_hat} b_hat: {b_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the predicted model fits the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y);\n",
    "plt.plot(X, y_hat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s a nice fit. Those two lines overlap pretty good, what did you expect? Pretty good for a couple of lines of code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
